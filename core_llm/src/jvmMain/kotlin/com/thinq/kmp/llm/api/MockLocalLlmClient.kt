package com.thinq.kmp.llm.api

import com.thinq.kmp.llm.error.LlmError
import kotlinx.coroutines.delay
import kotlinx.coroutines.flow.Flow
import kotlinx.coroutines.flow.flow

/**
 * Mock implementation of LocalLlmClient for desktop/JVM platforms.
 *
 * This implementation provides simulated responses for testing and development purposes.
 * It does not connect to any real LLM model.
 */
internal class MockLocalLlmClient : LocalLlmClient {

    override val capabilities: Set<LlmCapability> = setOf(
        LlmCapability.TEXT_GENERATION,
        LlmCapability.SUMMARIZATION,
        LlmCapability.REWRITING,
        LlmCapability.PROOFREADING,
        LlmCapability.STREAMING
    )

    override suspend fun isAvailable(): Boolean {
        // Mock is always available
        return true
    }

    override suspend fun generateText(request: LlmRequest): LlmResponse {
        // Simulate processing delay
        delay(500)

        val mockResponseText = buildMockResponse(request)

        return LlmResponse(
            text = mockResponseText,
            usage = TokenUsage(
                promptTokens = request.prompt.split(" ").size,
                completionTokens = mockResponseText.split(" ").size,
                totalTokens = request.prompt.split(" ").size + mockResponseText.split(" ").size
            )
        )
    }

    override fun streamText(request: LlmRequest): Flow<String> = flow {
        val mockResponseText = buildMockResponse(request)
        val words = mockResponseText.split(" ")

        // Emit words one by one to simulate streaming
        for (word in words) {
            delay(50) // Simulate streaming delay
            emit("$word ")
        }
    }

    private fun buildMockResponse(request: LlmRequest): String {
        val prompt = request.prompt.lowercase()

        return when {
            "summarize" in prompt || "summary" in prompt -> {
                "Mock summary: This is a simulated summary of the provided text. " +
                "The mock LLM has identified the key points and condensed them into this brief overview."
            }
            "rewrite" in prompt || "rephrase" in prompt -> {
                "Mock rewrite: Here is a simulated rewritten version of your text. " +
                "The mock LLM has rephrased the content while maintaining the original meaning."
            }
            "proofread" in prompt || "correct" in prompt -> {
                "Mock proofreading: Your text has been reviewed by the mock LLM. " +
                "No significant grammar or spelling errors were found in this simulation."
            }
            "explain" in prompt -> {
                "Mock explanation: This is a simulated explanation. " +
                "The mock LLM would provide a detailed breakdown of the concept in a real implementation."
            }
            else -> {
                "Mock response: This is a simulated response from the desktop mock LLM. " +
                "In a real implementation, this would be generated by an actual language model. " +
                "Your prompt was: \"${request.prompt.take(50)}${if (request.prompt.length > 50) "..." else ""}\""
            }
        }
    }
}
